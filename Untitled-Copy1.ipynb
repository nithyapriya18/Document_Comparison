{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import docx2txt\n",
    "from nltk.probability import FreqDist\n",
    "import math\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim import models as mlls\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "stopwords_en=stopwords.words(\"english\")\n",
    "from docx import Document\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    wordlist=nltk.word_tokenize(data)\n",
    "    text=[w.lower() for w in wordlist if w not in stopwords_en]\n",
    "    return text\n",
    "\n",
    "# text = ''\n",
    "bbc = preprocessing(docx2txt.process('buscrash_bbc.docx')) \n",
    "fox = preprocessing(docx2txt.process('buscrash_bbc.docx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency Count Method\n",
    "word_set=set(bbc).union(set(fox))\n",
    "\n",
    "freqd_bbc=FreqDist(bbc)\n",
    "bbc_length=len(bbc)\n",
    "bbc_count_dict=dict.fromkeys(word_set,0)\n",
    "for word in bbc:\n",
    "    bbc_count_dict[word] = freqd_bbc[word]\n",
    "    \n",
    "freqd_fox=FreqDist(fox)\n",
    "fox_length=len(fox)\n",
    "fox_count_dict=dict.fromkeys(word_set,0)\n",
    "for word in fox:\n",
    "    fox_count_dict[word] = freqd_fox[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Method\n",
    "#TF Calculations\n",
    "tf_freqd_bbc=FreqDist(bbc)\n",
    "bbc_length=len(bbc)\n",
    "tf_bbc_tf_dict=dict.fromkeys(word_set,0)\n",
    "for word in bbc:\n",
    "    tf_bbc_tf_dict[word] = tf_freqd_bbc[word]/bbc_length\n",
    "    \n",
    "tf_freqd_fox=FreqDist(fox)\n",
    "fox_length=len(fox)\n",
    "tf_fox_tf_dict=dict.fromkeys(word_set,0)\n",
    "for word in fox:\n",
    "    tf_fox_tf_dict[word] = tf_freqd_fox[word]/fox_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF Calculations\n",
    "text_idf_dict=dict.fromkeys(word_set,0)\n",
    "text_length=2 #2 docs\n",
    "for word in text_idf_dict.keys():\n",
    "    if word in bbc:\n",
    "        text_idf_dict[word]+=1\n",
    "    if word in fox:\n",
    "        text_idf_dict[word]+=1\n",
    "        \n",
    "for word,val in text_idf_dict.items():\n",
    "    text_idf_dict[word] =1+math.log(text_length/(float(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Calcculations\n",
    "#TFIDF of a word = (TF of the word) * (IDF of the word)\n",
    "bbc_tfidf_dict=dict.fromkeys(word_set,0)\n",
    "for word in bbc:\n",
    "    bbc_tfidf_dict[word]=(tf_bbc_tf_dict[word])*(text_idf_dict[word])\n",
    "    \n",
    "fox_tfidf_dict=dict.fromkeys(word_set,0)\n",
    "for word in fox:\n",
    "    fox_tfidf_dict[word]=(tf_fox_tf_dict[word])*(text_idf_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word embedding method\n",
    "taggeddocs=[]\n",
    "bbc=TaggedDocument(words=bbc, tags=[u'news_bbc'])\n",
    "taggeddocs.append(bbc)\n",
    "fox=TaggedDocument(words=fox, tags=[u'news_fox'])\n",
    "taggeddocs.append(fox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training epoch 0\n",
      "Now training epoch 20\n",
      "Now training epoch 40\n",
      "Now training epoch 60\n"
     ]
    }
   ],
   "source": [
    "#model building\n",
    "model=mlls.Doc2Vec(taggeddocs, dm=0, alpha=0.025,vector_size=20,min_alpha=0.025, min_count=0)\n",
    "\n",
    "#train\n",
    "for epoch in range (80):\n",
    "    if epoch%20==0:\n",
    "        print(\"Now training epoch %s\"%epoch)\n",
    "    model.train(taggeddocs,total_examples=model.corpus_count,epochs=model.epochs)\n",
    "    model.alpha-=0.02 #reduce learning rate\n",
    "    model.min_alpha=model.alpha #fix LR - no decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once the words in the text are vectorised, the similarity score between them is nothing but the ‘distance’ between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Index: 100.00%\n"
     ]
    }
   ],
   "source": [
    "#Distance Computation\n",
    "#Cosine Distance\n",
    "v1=list(bbc_tfidf_dict.values())\n",
    "v2=list(fox_tfidf_dict.values())\n",
    "similarity=1-nltk.cluster.cosine_distance(v1,v2)\n",
    "print(\"Similarity Index: {:4.2f}%\".format(similarity*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentfromdoc(filename):\n",
    "    data=Document(filename)\n",
    "    list1=list()\n",
    "    list2=list()\n",
    "    for p in data.paragraphs:\n",
    "        list1.append(p.text)\n",
    "    for val in list1:\n",
    "        tokens = nltk.sent_tokenize(val)\n",
    "        for t in tokens:\n",
    "            list2.append(t)\n",
    "    return list2\n",
    "    \n",
    "# bbc1 = Document('buscrash_bbc.docx')\n",
    "# fox1 = Document('buscrash_foxnews.docx')\n",
    "\n",
    "l_bbc=sentfromdoc('buscrash_bbc.docx')\n",
    "l_fox=sentfromdoc('buscrash_foxnews.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- buscrash_bbc.docx\n",
      "+++ buscrash_foxnews.docx\n",
      "@@ -1,30 +1,16 @@\n",
      "-At least 46 people, including 12 children, died when a bus crashed and caught fire in western Bulgaria, officials say.\n",
      "-The bus was registered in North Macedonia and most of those on board were tourists returning from a trip to Istanbul in Turkey.\n",
      "-It rammed a crash barrier on a motorway south-west of the capital Sofia at about 02:00 local time (00:00 GMT).\n",
      "-Seven people escaped from the bus and were taken to hospital with burns.\n",
      "-Four-year-old twin boys were among those killed in the crash.\n",
      "-Bulgarian Interior Minister Boyko Rashkov visited the \"terrifying scene\" and said the survivors had been badly burned.\n",
      "-The cause of the disaster was not immediately clear.\n",
      "-Bulgarian officials described how the bus swerved off the motorway and tore away a 50m (164ft) section of the crash barrier, but it was unclear if that was before or after it caught fire.\n",
      "-Pictures of the aftermath showed a section of the road where the barrier had been shorn off.\n",
      "-No other vehicles were involved in the accident.\n",
      "-The mayor of the nearby village of Pernik said the motorway was in poor condition on that section and there were often accidents in the area.\n",
      "-Macedonian Foreign Minister Bujar Osmani told reporters the coach party had been returning to the capital Skopje from a weekend holiday trip to Istanbul.\n",
      "-Bulgarian media said the bus had been travelling as part of a convoy of four buses and had stopped off at a petrol station near Sofia about an hour before the accident.\n",
      "-The other buses, which were a few minutes ahead, returned to North Macedonia safely.\n",
      "-Part of the motorway barrier was destroyed in the crash\n",
      "-The victims have not yet been officially named, but officials said they included 12 children, and many young people aged between 20 and 30.\n",
      "-A young couple who were due to be married were among those killed.\n",
      "-Macedonian reports said 27-year old Gazmend Ukali and Albina Beluli, 23, from the north-western town of Tetovo, had gone to Istanbul to celebrate Ukali's birthday.\n",
      "-Albania's foreign minister indicated that most, if not all, the passengers were ethnic Albanians from North Macedonia.\n",
      "-Macedonian Prime Minister Zoran Zaev said he had spoken to one of the survivors, who told him that passengers were asleep when the sound of an explosion woke them.\n",
      "-\"He and the other six survivors broke the windows of the bus and managed to escape and save themselves,\" Mr Zaev told reporters.\n",
      "-The bus belonged to Besa Trans, a travel company that organises trips in Europe.\n",
      "-Within hours of the crash, relatives of people who travelled to Turkey with Besa Trans last week gathered outside the firm's office in Skopje, anxiously looking for information.\n",
      "-Dzelal Bakiu told reporters in the Macedonian capital he was concerned for his nephew and had not heard from him since learning of the crash.\n",
      "-He tried to contact the travel agency but had not been able to get any information.\n",
      "-Bulgaria's interim Prime Minister Stefan Yanev described the incident as \"an enormous tragedy\".\n",
      "-\"Let's hope we learn lessons from this tragic incident and we can prevent such incidents in the future,\" he told reporters as he visited the crash site.\n",
      "-Investigative service chief Borislav Sarafov said \"human error by the driver or a technical malfunction are the two initial versions for the accident\".\n",
      "-The area around the site of Tuesday's incident on the Struma motorway was sealed off and footage from the scene showed the charred vehicle, gutted by the fire.\n",
      "-Mr Sarafov told reporters that it appeared both drivers had been killed in the crash so no-one was able to open the doors.\n",
      "+A bus crash in early Tuesday has killed at least 45 people, authorities said.\n",
      "+The bus, registered in North Macedonia, crashed around 2 a.m. and there were children among the victims, authorities said.\n",
      "+Seven people were taken to hospitals for treatment.\n",
      "+The cause of the crash was not immediately confirmed, but it appeared that the bus hit a highway guard rail, crashed and caught fire.\n",
      "+Officials said an investigation will be launched.\n",
      "+Bulgarian news agency Novinite said representatives from North Macedonia’s embassy visited a hospital where some of the victims were taken.\n",
      "+Photos taken shortly after the crash showed the bus engulfed in flames with plumes of thick, black smoke rising from the scene.\n",
      "+Bulgarian Caretaker Prime Minister Stefan Yanev visited the site of the crash and told reporters it was \"a huge tragedy.\"\n",
      "+\"I take this opportunity to send my condolences to the relatives of the victims,\" Yanev said.\n",
      "+\"Let’s hope we learn lessons from this tragic incident and we can prevent such incidents in the future.\"\n",
      "+North Macedonian Prime Minister Zoran Zaev told Bulgarian television channel bTV that he had spoken to one of the bus survivors.\n",
      "+\"One of the passengers told me that he was asleep and woke up from an explosion,\" Zaev told bTV, adding that the authorities will gather information that is \"important for the families of the dead and the survivors.\"\n",
      "+Oliver Varhelyi, a European Union Commissioner, sent his condolences to the families and friends of those affected by the crash.\n",
      "+\"Terrible news about the tragic bus accident in Bulgaria in early morning hours,\" Varhelyi wrote online.\n",
      "+\"My thoughts & condolences are with the families and friends of those who died as well as with the people and the authorities of North Macedonia.\"\n",
      "+In 2019, Bulgaria, an EU nation of 7 million, had the second-highest road fatality rate in the 27-nation bloc with 89 people killed per million population, according to European Commission data.\n"
     ]
    }
   ],
   "source": [
    "# Find and print the diff:\n",
    "for line in difflib.unified_diff(\n",
    "        l_bbc, l_fox, fromfile='buscrash_bbc.docx', \n",
    "        tofile='buscrash_foxnews.docx', lineterm=''):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentfromdoc(filename):\n",
    "    data=Document(filename)\n",
    "    list1=list()\n",
    "    list2=list()\n",
    "    for p in data.paragraphs:\n",
    "        list1.append(p.text)\n",
    "    for val in list1:\n",
    "        tokens = nltk.sent_tokenize(val)\n",
    "        for t in tokens:\n",
    "            list2.append(t)\n",
    "    return list2\n",
    "    \n",
    "# bbc1 = Document('buscrash_bbc.docx')\n",
    "# fox1 = Document('buscrash_foxnews.docx')\n",
    "\n",
    "l_bbc=sentfromdoc('buscrash_bbc.docx')\n",
    "l_fox=sentfromdoc('buscrash_foxnews.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(list_):\n",
    "    processed_features = []\n",
    "    for sentence in range(0, len(list_)):\n",
    "        processed_feature = re.sub(r'\\W', ' ', str(list_[sentence])) #special character removal\n",
    "        processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature) #single charracter\n",
    "        processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I) #multi space issues handled\n",
    "        processed_feature = re.sub(r'^b\\s+', '', processed_feature) #removing prefix b\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        processed_feature = processed_feature.lower()\n",
    "\n",
    "        processed_features.append(processed_feature)\n",
    "        \n",
    "#         documents = set(processed_features) #removing duplicates\n",
    "    word_token_sent=[]\n",
    "    for i in range(0,len(processed_features)):\n",
    "        word_token_sent.append(word_tokenize(processed_features[i]))\n",
    "    return processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_bbc=tokenizing(l_bbc)\n",
    "token_fox=tokenizing(l_fox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(file):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(file)\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_bbc=vectorize(token_bbc)\n",
    "vector_fox=vectorize(token_fox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are taking the 6th sentence from bbc to find its top 2 similar sentences in fox\n",
    "# vector_bbc.shape\n",
    "# vector_fox.shape\n",
    "# shape = np.maximum(vector_bbc.shape, vector_fox.shape)\n",
    "# shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 291 while Y.shape[1] == 172",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-fb5977bdf917>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Cosine Similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msimilarity_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_bbc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvector_fox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"comparing - \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_bbc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# input tweet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ana_envi\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1177\u001b[0m     \u001b[1;31m# to avoid recursive import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1179\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1181\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ana_envi\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ana_envi\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    159\u001b[0m         raise ValueError(\"Incompatible dimension for X and Y matrices: \"\n\u001b[0;32m    160\u001b[0m                          \"X.shape[1] == %d while Y.shape[1] == %d\" % (\n\u001b[1;32m--> 161\u001b[1;33m                              X.shape[1], Y.shape[1]))\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 291 while Y.shape[1] == 172"
     ]
    }
   ],
   "source": [
    "#Cosine Similarity\n",
    "\n",
    "similarity_index=cosine_similarity(vector_bbc[6:7],vector_fox)\n",
    "print(\"comparing - \", token_bbc[6]) # input tweet\n",
    "\n",
    "similarity_index=similarity_index.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now display top 5 most similar sentences using similarity index calculated in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n=6\n",
    "print (similarity_index[np.argsort(similarity_index)[-n:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sorted(range(len(similarity_index)), key = lambda sub: similarity_index[sub])[-n:] \n",
    "  \n",
    "# printing result \n",
    "print(\"Indices list of max N elements is : \" + str(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in res:\n",
    "    print(processed_features[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Euclidean_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "similarity_index=euclidean_distances(tfidf_matrix[6:7],tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_index=similarity_index.reshape(-1)\n",
    "n=6\n",
    "print (similarity_index[np.argsort(similarity_index)[-n:]])\n",
    "res = sorted(range(len(similarity_index)), key = lambda sub: similarity_index[sub])[-n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using euclidean distance we can see the result have almost similar words. Hence this approach is also called common word approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Indices list of max N elements is : \" + str(res)) \n",
    "for i in res:\n",
    "    print(processed_features[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- buscrash_bbc.docx\n",
      "+++ buscrash_foxnews.docx\n",
      "@@ -1,30 +1,16 @@\n",
      "-At least 46 people, including 12 children, died when a bus crashed and caught fire in western Bulgaria, officials say.\n",
      "-The bus was registered in North Macedonia and most of those on board were tourists returning from a trip to Istanbul in Turkey.\n",
      "-It rammed a crash barrier on a motorway south-west of the capital Sofia at about 02:00 local time (00:00 GMT).\n",
      "-Seven people escaped from the bus and were taken to hospital with burns.\n",
      "-Four-year-old twin boys were among those killed in the crash.\n",
      "-Bulgarian Interior Minister Boyko Rashkov visited the \"terrifying scene\" and said the survivors had been badly burned.\n",
      "-The cause of the disaster was not immediately clear.\n",
      "-Bulgarian officials described how the bus swerved off the motorway and tore away a 50m (164ft) section of the crash barrier, but it was unclear if that was before or after it caught fire.\n",
      "-Pictures of the aftermath showed a section of the road where the barrier had been shorn off.\n",
      "-No other vehicles were involved in the accident.\n",
      "-The mayor of the nearby village of Pernik said the motorway was in poor condition on that section and there were often accidents in the area.\n",
      "-Macedonian Foreign Minister Bujar Osmani told reporters the coach party had been returning to the capital Skopje from a weekend holiday trip to Istanbul.\n",
      "-Bulgarian media said the bus had been travelling as part of a convoy of four buses and had stopped off at a petrol station near Sofia about an hour before the accident.\n",
      "-The other buses, which were a few minutes ahead, returned to North Macedonia safely.\n",
      "-Part of the motorway barrier was destroyed in the crash\n",
      "-The victims have not yet been officially named, but officials said they included 12 children, and many young people aged between 20 and 30.\n",
      "-A young couple who were due to be married were among those killed.\n",
      "-Macedonian reports said 27-year old Gazmend Ukali and Albina Beluli, 23, from the north-western town of Tetovo, had gone to Istanbul to celebrate Ukali's birthday.\n",
      "-Albania's foreign minister indicated that most, if not all, the passengers were ethnic Albanians from North Macedonia.\n",
      "-Macedonian Prime Minister Zoran Zaev said he had spoken to one of the survivors, who told him that passengers were asleep when the sound of an explosion woke them.\n",
      "-\"He and the other six survivors broke the windows of the bus and managed to escape and save themselves,\" Mr Zaev told reporters.\n",
      "-The bus belonged to Besa Trans, a travel company that organises trips in Europe.\n",
      "-Within hours of the crash, relatives of people who travelled to Turkey with Besa Trans last week gathered outside the firm's office in Skopje, anxiously looking for information.\n",
      "-Dzelal Bakiu told reporters in the Macedonian capital he was concerned for his nephew and had not heard from him since learning of the crash.\n",
      "-He tried to contact the travel agency but had not been able to get any information.\n",
      "-Bulgaria's interim Prime Minister Stefan Yanev described the incident as \"an enormous tragedy\".\n",
      "-\"Let's hope we learn lessons from this tragic incident and we can prevent such incidents in the future,\" he told reporters as he visited the crash site.\n",
      "-Investigative service chief Borislav Sarafov said \"human error by the driver or a technical malfunction are the two initial versions for the accident\".\n",
      "-The area around the site of Tuesday's incident on the Struma motorway was sealed off and footage from the scene showed the charred vehicle, gutted by the fire.\n",
      "-Mr Sarafov told reporters that it appeared both drivers had been killed in the crash so no-one was able to open the doors.\n",
      "+A bus crash in early Tuesday has killed at least 45 people, authorities said.\n",
      "+The bus, registered in North Macedonia, crashed around 2 a.m. and there were children among the victims, authorities said.\n",
      "+Seven people were taken to hospitals for treatment.\n",
      "+The cause of the crash was not immediately confirmed, but it appeared that the bus hit a highway guard rail, crashed and caught fire.\n",
      "+Officials said an investigation will be launched.\n",
      "+Bulgarian news agency Novinite said representatives from North Macedonia’s embassy visited a hospital where some of the victims were taken.\n",
      "+Photos taken shortly after the crash showed the bus engulfed in flames with plumes of thick, black smoke rising from the scene.\n",
      "+Bulgarian Caretaker Prime Minister Stefan Yanev visited the site of the crash and told reporters it was \"a huge tragedy.\"\n",
      "+\"I take this opportunity to send my condolences to the relatives of the victims,\" Yanev said.\n",
      "+\"Let’s hope we learn lessons from this tragic incident and we can prevent such incidents in the future.\"\n",
      "+North Macedonian Prime Minister Zoran Zaev told Bulgarian television channel bTV that he had spoken to one of the bus survivors.\n",
      "+\"One of the passengers told me that he was asleep and woke up from an explosion,\" Zaev told bTV, adding that the authorities will gather information that is \"important for the families of the dead and the survivors.\"\n",
      "+Oliver Varhelyi, a European Union Commissioner, sent his condolences to the families and friends of those affected by the crash.\n",
      "+\"Terrible news about the tragic bus accident in Bulgaria in early morning hours,\" Varhelyi wrote online.\n",
      "+\"My thoughts & condolences are with the families and friends of those who died as well as with the people and the authorities of North Macedonia.\"\n",
      "+In 2019, Bulgaria, an EU nation of 7 million, had the second-highest road fatality rate in the 27-nation bloc with 89 people killed per million population, according to European Commission data.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>among</th>\n",
       "      <th>and</th>\n",
       "      <th>around</th>\n",
       "      <th>authorities</th>\n",
       "      <th>board</th>\n",
       "      <th>bus</th>\n",
       "      <th>children</th>\n",
       "      <th>crashed</th>\n",
       "      <th>from</th>\n",
       "      <th>in</th>\n",
       "      <th>...</th>\n",
       "      <th>the</th>\n",
       "      <th>there</th>\n",
       "      <th>those</th>\n",
       "      <th>to</th>\n",
       "      <th>tourists</th>\n",
       "      <th>trip</th>\n",
       "      <th>turkey</th>\n",
       "      <th>victims</th>\n",
       "      <th>was</th>\n",
       "      <th>were</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_bbc</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_fox</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         among  and  around  authorities  board  bus  children  crashed  from  \\\n",
       "doc_bbc      0    1       0            0      1    1         0        0     1   \n",
       "doc_fox      1    1       1            1      0    1         1        1     0   \n",
       "\n",
       "         in  ...  the  there  those  to  tourists  trip  turkey  victims  was  \\\n",
       "doc_bbc   2  ...    1      0      1   1         1     1       1        0    1   \n",
       "doc_fox   1  ...    2      1      0   0         0     0       0        1    0   \n",
       "\n",
       "         were  \n",
       "doc_bbc     1  \n",
       "doc_fox     1  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_=list()\n",
    "documents=[l_bbc[1],l_fox[1]]\n",
    "\n",
    "\n",
    "# Scikit Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Create the Document Term Matrix\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer()\n",
    "sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names(), \n",
    "                  index=['doc_bbc', 'doc_fox'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.46829291]\n",
      " [0.46829291 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.1\n",
      "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "# upgrade gensim if you can't import softcossim\n",
    "from gensim import matutils\n",
    "from gensim import corpora\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "print(gensim.__version__)\n",
    "#> '3.6.0'\n",
    "\n",
    "# Download the FastText model\n",
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'similarity_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-003ecd104044>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Prepare the similarity matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msimilarity_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfasttext_model300\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonzero_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyedVectors' object has no attribute 'similarity_matrix'"
     ]
    }
   ],
   "source": [
    "# Prepare a dictionary and a corpus.\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])\n",
    "\n",
    "# Convert the sentences into bag-of-words vectors.\n",
    "sent_1 = dictionary.doc2bow(simple_preprocess(doc_bbc))\n",
    "sent_2 = dictionary.doc2bow(simple_preprocess(doc_fox))\n",
    "\n",
    "\n",
    "sentences = [sent_1, sent_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute soft cosine similarity\n",
    "print(softcossim(sent_1, sent_2, similarity_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_soft_cossim_matrix(sentences):\n",
    "    len_array = np.arange(len(sentences))\n",
    "    xx, yy = np.meshgrid(len_array, len_array)\n",
    "    cossim_mat = pd.DataFrame([[round(softcossim(sentences[i],sentences[j], similarity_matrix) ,2) for i, j in zip(x,y)] for y, x in zip(xx, yy)])\n",
    "    return cossim_mat\n",
    "\n",
    "soft_cosine_similarity_matrix(sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
